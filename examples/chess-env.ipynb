{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | nb_init | Set current dir to chess\n",
      "INFO | nb_init | You are using Python 3.10.10 (main, Sep 14 2023, 16:59:47) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n"
     ]
    }
   ],
   "source": [
    "from shark.utils import nb_init\n",
    "nb_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chess environment for Reinforcement Learning\n",
    "\n",
    "Regardless of the RL Agent we wish to train, we need a Chess Environment with which this Agent can play. See also [this popular libray](https://gymnasium.farama.org/index.html) for further reference.\n",
    "\n",
    "As this repo is strongly based on TorchRL, we will extend their [base class](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html) to create this chess environment.\n",
    "\n",
    "This notebook will show you what that is and how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a chess environment\n",
    "\n",
    "To create a chess environment, all you have to do is to import the correct class, and initialize an instance of that class.\n",
    "\n",
    "However, let's also see some additional input parameters we can pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function __init__ in module shark.env.chess:\n",
      "\n",
      "__init__(self, engine_path: str = None, time: float = 5, depth: int = 20, flatten_state: bool = False, play_as: str = 'white', play_vs_engine: bool = True, mate_amplifier: float = 10, softmax: bool = True, worst_reward: float = -1000.0, illegal_amplifier: float = 100, lose_on_illegal_move: bool = True, use_one_hot: bool = True, from_engine: bool = False, **kwargs: Any) -> None\n",
      "    Args:\n",
      "        engine_path (str):\n",
      "            Path to chess engine. This class needs a usable chess engine.\n",
      "            For example: `stockfish`.\n",
      "            If not passed, this class will read from the `CHESS_ENGINE_EXECUTABLE` environment variable.\n",
      "            If not set, an error will be raised.\n",
      "            Please make sure to install a chess engine like Stockfish, and pass the correct\n",
      "            installation path here.\n",
      "    \n",
      "        time (float, optional):\n",
      "            Timeout value in seconds for engine.\n",
      "            When the chess engine is called to validate a position or play a move, this will be the timeout for that.\n",
      "            Defaults to `5`.\n",
      "    \n",
      "        depth (int, optional):\n",
      "            Depth value for chess engine. When the chess engine is called to validate a position or play a move, this will be depth of the search tree.\n",
      "            Defaults to `20`.\n",
      "    \n",
      "        flatten_state (bool, optional):\n",
      "            Whether to flatten or not the state tensor.\n",
      "            The state tensor has shape `(8, 8, 13)`, where 8 x 8 is the chess board, and for each square\n",
      "            we have 13 possible values: 6 possible white pieces, 6 possible black pieces, empty square.\n",
      "            If `True`, this `(8, 8, 13)` tensor will be flattened to `(8 * 8 * 13,)`.\n",
      "            Consider not doing this.\n",
      "            Defaults to `False`.\n",
      "    \n",
      "        play_as (str, optional):\n",
      "            White or black. Defaults to `\"white\"`.\n",
      "    \n",
      "        play_vs_engine (bool, optional):\n",
      "            Defaults to `True`. The `False` value is reserved for future use. Currently unsupported.\n",
      "            In the future, you will be able to play against another RL agent.\n",
      "    \n",
      "        mate_amplifier (float, optional):\n",
      "            Reward amplifier when mate happens.\n",
      "            Defaults to `100`.\n",
      "    \n",
      "        softmax (bool, optional):\n",
      "            If `True`, action values go through a softmax operation.\n",
      "            This is useful, yet not necessary, when your agent outputs logits.\n",
      "            This is mandatory if you want this environment to automatically discard any illegal chess\n",
      "            move that your agent may select.\n",
      "            Defaults to `True`.\n",
      "    \n",
      "        worst_reward (float, optional):\n",
      "            Value for the worst reward possible, e.g. when losing a game. Defaults to `WORST_REWARD`.\n",
      "    \n",
      "        illegal_amplifier (float, optional):\n",
      "            Reward amplifier when an illegal move is selected.\n",
      "            Defaults to `1000`.\n",
      "    \n",
      "        lose_on_illegal_move (bool, optional): Defaults to `True`.\n",
      "            Whether agent loses when an illegal move is played.\n",
      "            If `False`, given that the action tensor is a vector of probabilities,\n",
      "            the first legal action value will be enforced from it,\n",
      "            giving priority to actions with higher probability.\n",
      "            If `True`, the game is automatically lost when an illegal move is selected.\n",
      "            Use this in combination with the `illegal_amplifier` parameter.\n",
      "    \n",
      "        use_one_hot (bool, optional):\n",
      "            Whether to use one-hot vectors for state.\n",
      "            Currently, this must be `True`, as `False` is not yet supported.\n",
      "            Defaults to `True`.\n",
      "    \n",
      "        from_engine (bool, optional):\n",
      "            Whetehr to sample moves from engine or randomly in the `sample()` method.\n",
      "            The `sample()` method is not actively used when playing, only when doing rollouts.\n",
      "            This is a rather irrelevant parameter.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from shark.env import ChessEnv\n",
    "\n",
    "help(ChessEnv.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ChessEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reset the state (start a new game), and see what the output is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset state: TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([8, 8, 13]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print(f\"Reset state: {state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state variable is a `TensorDict`, basically a `dict` of `Tensor` objects.\n",
    "\n",
    "The fields are:\n",
    "* `done`: whether the game is over or not. Should not be, as we just started one.\n",
    "* `observation`: the current state, thus the chess board. If playing as \"black\", then \"white\" (played by the chess engine) moved first and a move has already happened on the board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a random agent, and start playing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.collectors import RandomPolicy\n",
    "\n",
    "actor = RandomPolicy(env.action_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = actor(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([4032]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([8, 8, 13]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each turn there are `4032` possible movements: going from a square on the board to another square on board.\n",
    "\n",
    "Clearly, not all movements are legal: the starting square may be empty, or occupied by an opponent's piece, or the landing square may not be reachable by the selected piece.\n",
    "\n",
    "You can also select a random move as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = env.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([4032]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "    batch_size=torch.Size([]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above method has the advantage that it will either call the engine to sample a move,\n",
    "or sample a random move only among the possible legal ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `torchrl` validators to see that the env is correctly set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 20:23:56,579 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "from torchrl.envs import check_env_specs\n",
    "\n",
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also perform a rollout, that is kind of randomly playing for a particular number of steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([3, 4032]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([3, 8, 8, 13]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([3]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([3, 8, 8, 13]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([3]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = env.rollout(3)\n",
    "td"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to play, you can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = env.reset()\n",
    "while not env.is_game_over():\n",
    "    action = actor(td) # or env.sample()\n",
    "    td = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Outcome(termination=<Termination.CHECKMATE: 1>, winner=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.board.outcome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chess-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
